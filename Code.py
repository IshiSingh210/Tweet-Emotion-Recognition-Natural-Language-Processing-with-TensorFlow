# -*- coding: utf-8 -*-
"""Tweet Emotion Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15utzUbsn81zWpqz8lVpuhHCZtwhK_HEZ

# Tweet Emotion Recognition: Natural Language Processing with TensorFlow

## Setup and Imports

1. Installing Hugging Face's nlp package
2. Importing libraries
"""

!pip install nlp

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nlp
import random


def show_history(h):
    epochs_trained = len(h.history['loss'])
    plt.figure(figsize=(16, 6))

    plt.subplot(1, 2, 1)
    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')
    plt.ylim([0., 1.])
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


def show_confusion_matrix(y_true, y_pred, classes):
    from sklearn.metrics import confusion_matrix

    cm = confusion_matrix(y_true, y_pred, normalize='true')

    plt.figure(figsize=(8, 8))
    sp = plt.subplot(1, 1, 1)
    ctx = sp.matshow(cm)
    plt.xticks(list(range(0, 6)), labels=classes)
    plt.yticks(list(range(0, 6)), labels=classes)
    plt.colorbar(ctx)
    plt.show()


print('Using TensorFlow version', tf.__version__)

"""## Importing Data

1. Importing the Tweet Emotion dataset
2. Creating train, validation and test sets
3. Extracting tweets and labels from the examples
"""

!pip install datasets
from datasets import load_dataset
dataset = load_dataset('emotion')

dataset

train = dataset['train']
val = dataset['validation']
test = dataset['test']

DATA = {0: "sadness", 2:"love", 1:"joy", 3:"anger", 4:"fear", 5:"surprise"}

def get_tweets(data):
  tweets = [x['text'] for x in data]
  labels = [x['label'] for x in data]
  return tweets, labels

tweets, labels = get_tweets(train)

tweets[0], labels[0]

"""## Tokenizer

1. Tokenizing the tweets
"""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')
tokenizer.fit_on_texts(tweets)

tokenizer.texts_to_sequences([tweets[0]])

tweets[0]

"""## Padding and Truncating Sequences

1. Checking length of the tweets
2. Creating padded sequences
"""

lengths = [len(t.split(' ')) for t in tweets]
plt.hist(lengths, bins=len(set(lengths)))
plt.show()

maxlen = 50

from tensorflow.keras.preprocessing.sequence import pad_sequences

def get_sequences(tokenizer, tweets):
  sequences = tokenizer.texts_to_sequences(tweets)
  padded = pad_sequences(sequences, truncating='post', maxlen=maxlen, padding='post')
  return padded

padded_train_seq = get_sequences(tokenizer, tweets)
padded_train_seq[0]

"""## Preparing the Labels

1. Creating classes to index and index to classes dictionaries
2. Converting text labels to numeric labels
"""

classes = set(labels)
print(classes)

plt.hist(labels, bins=11)
plt.show()

classes = list(classes)
names_to_ids = lambda labels: np.array([classes[x] for x in labels])

train_labels = names_to_ids(labels)
print(train_labels[0])







"""## Creating the Model

1. Creating the model
2. Compiling the model
"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(10000, 16),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),
    tf.keras.layers.Dense(6, activation='softmax')
])

model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.build(input_shape=(None, maxlen))

model.summary()

"""## Training the Model

1. Preparing a validation set
2. Training the model
"""

val_tweets, val_labels = get_tweets(val)
val_seq = get_sequences(tokenizer, val_tweets)
val_labels = names_to_ids(val_labels)

val_tweets[0], val_labels[0]

h = model.fit(
    padded_train_seq, train_labels,
    validation_data=(val_seq, val_labels),
    epochs=20,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)
    ]
)

"""## Evaluating the Model

1. Visualizing training history
2. Prepraring a test set
3. A look at individual predictions on the test set
4. A look at all predictions on the test set
"""

show_history(h)

test_tweets, test_labels = get_tweets(test)
test_seq = get_sequences(tokenizer, test_tweets)
test_labels = names_to_ids(test_labels)

_ = model.evaluate(test_seq, test_labels)

i = random.randint(0, len(test_labels)-1)
print('Sentence:', test_tweets[i])
print('Emotion:', classes[test_labels[i]])

preds = model.predict(test_seq)

show_confusion_matrix(test_labels, preds.argmax(axis=1), list(classes))
print(DATA)
